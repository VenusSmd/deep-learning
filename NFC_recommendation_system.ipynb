{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9432373-ff3c-490e-9874-645ed27c26b1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project demonstrates the implementation of a Neural Collaborative Filtering (NCF) model for a movie recommendation system using the MovieLens dataset. Collaborative filtering is a popular method in recommendation systems that makes predictions about user preferences based on the preferences of other users. By integrating neural network architectures, NCF models can capture the complex non-linear relationships between users and items (movies, in this case), leading to more accurate and personalized recommendations.\n",
    "\n",
    "The purpose of this project is to:\n",
    "\n",
    "Introduce the concept and implementation of Neural Collaborative Filtering.\n",
    "\n",
    "Demonstrate how to preprocess data for a recommendation system.\n",
    "\n",
    "Showcase the building, training, and evaluation of an NCF model using TensorFlow's Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74bdc3aa-e55c-4613-ae1f-8efe81d98ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d000cb4-6db7-43b8-8441-95b9db3ddc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# MovieLens 100K dataset\n",
    "\n",
    "df = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "\n",
    "# Encode user and movie IDs to create dense indices\n",
    "user_encoder = LabelEncoder()\n",
    "df['userId'] = user_encoder.fit_transform(df['userId'])\n",
    "movie_encoder = LabelEncoder()\n",
    "df['movieId'] = movie_encoder.fit_transform(df['movieId'])\n",
    "\n",
    "# Define the number of unique users and movies\n",
    "num_users = df['userId'].nunique()\n",
    "num_movies = df['movieId'].nunique()\n",
    "\n",
    "# Prepare training and testing datasets\n",
    "X = df[['userId', 'movieId']].values\n",
    "\n",
    "# Normalization: Neural networks generally perform better when the input data\n",
    "# is normalized or standardized. This is because normalization helps in speeding up the learning process\n",
    "# and reaching convergence faster. It ensures that the scale of the output matches the scale of the activation function.\n",
    "y = df['rating'].values / 5.0 # Scale ratings to [0, 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4558c-7aae-404b-a194-2f9c40d0da15",
   "metadata": {},
   "source": [
    "## Defining the Neural Collaborative Filtering Model\n",
    "\n",
    "In this step, we will define our Neural Collaborative Filtering model using Keras. The model will have the following components:\n",
    "- **Embedding Layers**: To convert user and movie IDs into dense vectors of fixed size.\n",
    "- **Flatten Layers**: To convert the embeddings into a format suitable for input to the dense layers.\n",
    "- **Concatenate Layer**: To merge the user and movie embeddings.\n",
    "- **Dense Layers**: A series of dense layers that learn to predict user ratings from the concatenated embeddings.\n",
    "- **Output Layer**: A single neuron with a sigmoid activation function to predict the scaled rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "383b0af6-ff49-41bd-93f6-74f2b3666639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "embedding_size = 50\n",
    "\n",
    "# User and Movie Input layers\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "movie_input = Input(shape=(1,), name='movie_input')\n",
    "\n",
    "# Embedding layers\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, name='user_embedding')(user_input)\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_size, name='movie_embedding')(movie_input)\n",
    "\n",
    "# Flatten the embeddings\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "movie_vec = Flatten(name='flatten_movies')(movie_embedding)\n",
    "\n",
    "# Concatenate the flattened embeddings\n",
    "concat = Concatenate()([user_vec, movie_vec])\n",
    "\n",
    "# Dense layers\n",
    "dense_1 = Dense(128, activation='relu')(concat)\n",
    "output = Dense(1, activation='sigmoid')(dense_1)  # Predicts the scaled rating\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87f4fd-4da1-43b4-9d09-4eb411ab7359",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "We use mean squared error as the loss function and Adam as the optimizer. The model will be trained for a predefined number of epochs, and we will also use a validation split to monitor its performance on unseen data during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "296d3af7-f597-4bcb-a1ad-c78bf2889bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0311\n",
      "Epoch 2/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0272 - val_loss: 0.0308\n",
      "Epoch 3/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0248 - val_loss: 0.0309\n",
      "Epoch 4/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0227 - val_loss: 0.0316\n",
      "Epoch 5/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0210 - val_loss: 0.0321\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train[:, 0], X_train[:, 1]], y_train, batch_size=64, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276dbd9-130f-4d4f-a19f-81418f978af0",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Decreasing Training Loss: Training loss is decreasing with each epoch, from 0.0399 in the first epoch to 0.0212 by the fifth epoch. This indicates that the model is learning from the training data and improving its predictions over time.\n",
    "\n",
    "Validation Loss Behavior: The validation loss decreases initially, reaching its lowest at the second epoch (0.0308), but then it starts to increase slightly in epochs 3,4 and 5. This could be an early sign of overfitting, where the model performs better on the training data but slightly worse on unseen data (validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a303992-276b-4537-91dd-9dae9cf5bf26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0188 - val_loss: 0.0326\n",
      "Epoch 2/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0168 - val_loss: 0.0343\n",
      "Epoch 3/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0148 - val_loss: 0.0352\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train[:, 0], X_train[:, 1]], y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf828b8-53e3-49b4-9759-2d6ed45a5fe1",
   "metadata": {},
   "source": [
    "## Early Stopping Effectiveness: \n",
    "The early stopping callback functioned as intended. It stopped the training process when it detected that the validation loss was no longer decreasing (and actually began to increase), which occurred after the third epoch. This helped prevent further overfitting by not allowing the model to continue learning from the training data to the point where its performance on the validation set could have worsened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65aa05df-a108-4bb0-9c4f-2f9f48063879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0173 - val_loss: 0.0339\n",
      "Epoch 2/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0154 - val_loss: 0.0352\n",
      "Epoch 3/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0135 - val_loss: 0.0370\n",
      "Epoch 4/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0116 - val_loss: 0.0370\n",
      "Epoch 5/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0105 - val_loss: 0.0382\n"
     ]
    }
   ],
   "source": [
    "# adjusting the learning rate\n",
    "optimizer = Adam(learning_rate=0.001)  # Try different learning rates\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Training with a different batch size\n",
    "history = model.fit([X_train[:, 0], X_train[:, 1]], y_train, batch_size=32, epochs=5, validation_split=0.2)  # Adjusted batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "078840a4-3fc0-41b1-8c04-9e7c2ae5ee6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0405 - val_loss: 0.0317\n",
      "Epoch 2/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0295 - val_loss: 0.0309\n",
      "Epoch 3/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0279 - val_loss: 0.0307\n",
      "Epoch 4/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0267 - val_loss: 0.0307\n",
      "Epoch 5/5\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0261 - val_loss: 0.0308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming these are your existing parameters and input layers\n",
    "embedding_size = 50\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "movie_input = Input(shape=(1,), name='movie_input')\n",
    "\n",
    "# Embedding layers\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, name='user_embedding')(user_input)\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_size, name='movie_embedding')(movie_input)\n",
    "\n",
    "# Flatten the embeddings\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "movie_vec = Flatten(name='flatten_movies')(movie_embedding)\n",
    "\n",
    "# Concatenate the flattened embeddings\n",
    "concat = Concatenate()([user_vec, movie_vec])\n",
    "\n",
    "# Adding dropout after concatenation\n",
    "concat_dropout = Dropout(0.5)(concat)  # Dropout layer with a 50% dropout rate\n",
    "\n",
    "# Dense layer after dropout\n",
    "dense_1 = Dense(64, activation='relu')(concat_dropout)  # Reduced the number of neurons from previous examples to 64\n",
    "\n",
    "\n",
    "dense_dropout = Dropout(0.4)(dense_1)  # Additional dropout layer\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(dense_dropout)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "optimizer = Adam(learning_rate=0.001)  # Adjust learning rate as needed\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Add early_stopping to your model.fit() callbacks\n",
    "history = model.fit(\n",
    "    [X_train[:, 0], X_train[:, 1]], y_train,\n",
    "    batch_size=32,\n",
    "    epochs=5,  # Increased epochs\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50781285-0cf1-41c5-af26-3d6af2e7748b",
   "metadata": {},
   "source": [
    "## Evaluating the Model Again\n",
    "After retraining the model with the adjustments, evaluating its performance on the test set again to see if the changes helped reduce overfitting and improved generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad39108-3b12-408a-bf67-2a3c047bcf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0375\n",
      "Test Loss: 0.037041060626506805\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(f'Test Loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252aca22-2a47-4ef8-b676-24a07cb6947f",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf45c48e-a33f-48d6-a77d-ea86008e0e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step\n",
      "Test RMSE: 0.19242336325658457\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Test RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfee4d-a436-4be7-b5ce-7814ac66ec2a",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "An RMSE of 0.1924 indicates that, on average, the model's predictions deviate from the actual ratings by about 0.1924 points on the normalized scale (since we normalized ratings to be between 0 and 1). Considering that the original ratings likely range from 1 to 5, this level of error might be quite acceptable, depending on the specifics of your application and the level of precision required for this case .\n",
    "\n",
    "RMSE is particularly useful for understanding the size of the error in the same units as the predicted quantity, making it easier to interpret in the context of the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef659d-1f7b-4c47-9a7c-3e376acde599",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "This project has successfully demonstrated the construction and evaluation of a Neural Collaborative Filtering (NCF) model using the MovieLens dataset, achieving promising results with a test RMSE of approximately 0.1924. This metric indicates a relatively low average error in the model's rating predictions, suggesting that the NCF model can effectively capture user preferences and predict unseen movie ratings. The incorporation of techniques such as embedding layers, dropout for regularization, and early stopping to prevent overfitting has shown to be beneficial in refining the model's performance.\n",
    "\n",
    "To further enhance the model, several strategies can be considered. First, fine-tuning the model's hyperparameters, including the learning rate, embedding size, and dropout rates, could yield improvements in accuracy. Exploring different model architectures or adding additional layers could also help in capturing more complex user-item interactions. Moreover, incorporating additional data, such as movie genres or user demographics, might enrich the model's understanding and lead to more personalized recommendations.\n",
    "\n",
    "In conclusion, while the model has shown promising results, the path to optimizing a recommendation system is iterative and requires continuous experimentation and adaptation. The insights gained from this project lay a solid foundation for future enhancements, driving towards a more accurate and user-centric recommendation system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
